{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this notebook is to demonstrate `sklearn` dataset API\n",
    "\n",
    "Recall that it has 3 APIs:\n",
    "1. **Loaders** (`load_*`) loads small standard datasets bundled with `sklearn` (these need not be downloaded separately).\n",
    "2. **Fetchers** (`fetch_*`) fetches large datasets from the internet and loads them in memory.\n",
    "3. **Generators** (`genrate_*`) generates controlled synthetic datasets (as required by us).\n",
    "\n",
    "*Loaders* and *fetchers* return a `bunch` object and *generators* return a tuple - (feature matrix, label vector (or matrix))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `Bunch` object `data` which is a dictionary like object with following attributes:\n",
    "\n",
    "- `data`, which has the feature matrix\n",
    "- `target`, which is a label vector\n",
    "- `feature_names`, contains the names of the feaures\n",
    "- `target_names`, contains the names of the classes\n",
    "- `DESCR` has the full description of the dataset\n",
    "- `filename` has the path to the location of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access them one by one and examine their contents. For example, we can acces `feature_names` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.feature_names\n",
    "# We can see the names of the features in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the names of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The featrue matrix can be accessed using `iris_data.data`. Let's look at the first five examples in feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.data[:5]\n",
    "# We can observe 4 features per example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine the shape of the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.data.shape\n",
    "# We can see that there are 150 examples with 4 features each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we examine the label vector and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.target\n",
    "# There are 50 examples each from 3 classes: 0, 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read additional documentationn about `load_iris` in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_iris in module sklearn.datasets._base:\n",
      "\n",
      "load_iris(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the iris dataset (classification).\n",
      "    \n",
      "    The iris dataset is a classic and very easy multi-class classification\n",
      "    dataset.\n",
      "    \n",
      "    =================   ==============\n",
      "    Classes                          3\n",
      "    Samples per class               50\n",
      "    Samples total                  150\n",
      "    Dimensionality                   4\n",
      "    Features            real, positive\n",
      "    =================   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <iris_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "        below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (150, 4)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (150,)\n",
      "            The classification target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        target_names: list\n",
      "            The names of target classes.\n",
      "        frame: DataFrame of shape (150, 5)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        filename: str\n",
      "            The path to the location of the data.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed two wrong data points according to Fisher's paper.\n",
      "            The new version is the same as in R, but not as in the UCI\n",
      "            Machine Learning Repository.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let's say you are interested in the samples 10, 25, and 50, and want to\n",
      "    know their class name.\n",
      "    \n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> data = load_iris()\n",
      "    >>> data.target[[10, 25, 50]]\n",
      "    array([0, 0, 1])\n",
      "    >>> list(data.target_names)\n",
      "    ['setosa', 'versicolor', 'virginica']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can load and examinfe different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain feature matrix and label (as tuples) from `load_iris` and other loaders in general by setting the `return_X_y` argument to `True` (which is `False` by default to return a `Bunch` object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (150, 4)\n",
      "Shape of label vector: (150,)\n"
     ]
    }
   ],
   "source": [
    "iris_feature_matrix, iris_label_vector = load_iris(return_X_y= True)\n",
    "\n",
    "print('Shape of feature matrix:', iris_feature_matrix.shape)\n",
    "print('Shape of label vector:', iris_label_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load Iris dataset as a dateframe, we set the `as_frame` argument to `True`, while loading it and returning as `(X,y)` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_iris(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes_data = load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional details about this loader can be accessed from the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_diabetes in module sklearn.datasets._base:\n",
      "\n",
      "load_diabetes(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the diabetes dataset (regression).\n",
      "    \n",
      "    ==============   ==================\n",
      "    Samples total    442\n",
      "    Dimensionality   10\n",
      "    Features         real, -.2 < x < .2\n",
      "    Targets          integer 25 - 346\n",
      "    ==============   ==================\n",
      "    \n",
      "    Read more in the :ref:`User Guide <diabetes_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (442, 10)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (442,)\n",
      "            The regression target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        frame: DataFrame of shape (442, 11)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        data_filename: str\n",
      "            The path to the location of the data.\n",
      "        target_filename: str\n",
      "            The path to the location of the target.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabetes_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the feature matrix\n",
    "diabetes_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of label vector\n",
    "diabetes_data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,\n",
       "        -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, -0.02632783, -0.00844872,\n",
       "        -0.01916334,  0.07441156, -0.03949338, -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, -0.00567061, -0.04559945,\n",
       "        -0.03419447, -0.03235593, -0.00259226,  0.00286377, -0.02593034],\n",
       "       [-0.08906294, -0.04464164, -0.01159501, -0.03665645,  0.01219057,\n",
       "         0.02499059, -0.03603757,  0.03430886,  0.02269202, -0.00936191],\n",
       "       [ 0.00538306, -0.04464164, -0.03638469,  0.02187235,  0.00393485,\n",
       "         0.01559614,  0.00814208, -0.00259226, -0.03199144, -0.04664087]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the first 5 examples from the feature matrix\n",
    "diabetes_data.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_data.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_digits in module sklearn.datasets._base:\n",
      "\n",
      "load_digits(*, n_class=10, return_X_y=False, as_frame=False)\n",
      "    Load and return the digits dataset (classification).\n",
      "    \n",
      "    Each datapoint is a 8x8 image of a digit.\n",
      "    \n",
      "    =================   ==============\n",
      "    Classes                         10\n",
      "    Samples per class             ~180\n",
      "    Samples total                 1797\n",
      "    Dimensionality                  64\n",
      "    Features             integers 0-16\n",
      "    =================   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <digits_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_class : int, default=10\n",
      "        The number of classes to return. Between 0 and 10.\n",
      "    \n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (1797, 64)\n",
      "            The flattened data matrix. If `as_frame=True`, `data` will be\n",
      "            a pandas DataFrame.\n",
      "        target: {ndarray, Series} of shape (1797,)\n",
      "            The classification target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        target_names: list\n",
      "            The names of target classes.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "        frame: DataFrame of shape (1797, 65)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        images: {ndarray} of shape (1797, 8, 8)\n",
      "            The raw image data.\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "    https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    To load the data and visualize the images::\n",
      "    \n",
      "        >>> from sklearn.datasets import load_digits\n",
      "        >>> digits = load_digits()\n",
      "        >>> print(digits.data.shape)\n",
      "        (1797, 64)\n",
      "        >>> import matplotlib.pyplot as plt #doctest: +SKIP\n",
      "        >>> plt.gray() #doctest: +SKIP\n",
      "        >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n",
      "        >>> plt.show() #doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "help(load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 1797\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(digits_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.data.shape\n",
    "# 1797 examples with 64 features each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
       "         9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
       "        15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
       "         0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
       "        16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  4., 15., 12.,  0.,  0.,  0.,  0.,  3., 16., 15.,\n",
       "        14.,  0.,  0.,  0.,  0.,  8., 13.,  8., 16.,  0.,  0.,  0.,  0.,\n",
       "         1.,  6., 15., 11.,  0.,  0.,  0.,  1.,  8., 13., 15.,  1.,  0.,\n",
       "         0.,  0.,  9., 16., 16.,  5.,  0.,  0.,  0.,  0.,  3., 13., 16.,\n",
       "        16., 11.,  5.,  0.,  0.,  0.,  0.,  3., 11., 16.,  9.,  0.],\n",
       "       [ 0.,  0.,  7., 15., 13.,  1.,  0.,  0.,  0.,  8., 13.,  6., 15.,\n",
       "         4.,  0.,  0.,  0.,  2.,  1., 13., 13.,  0.,  0.,  0.,  0.,  0.,\n",
       "         2., 15., 11.,  1.,  0.,  0.,  0.,  0.,  0.,  1., 12., 12.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1., 10.,  8.,  0.,  0.,  0.,  8.,  4.,\n",
       "         5., 14.,  9.,  0.,  0.,  0.,  7., 13., 13.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  8.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1., 13.,  6.,  2.,  2.,  0.,  0.,  0.,\n",
       "         7., 15.,  0.,  9.,  8.,  0.,  0.,  5., 16., 10.,  0., 16.,  6.,\n",
       "         0.,  0.,  4., 15., 16., 13., 16.,  1.,  0.,  0.,  0.,  0.,  3.,\n",
       "        15., 10.,  0.,  0.,  0.,  0.,  0.,  2., 16.,  4.,  0.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pixel_0_0',\n",
       " 'pixel_0_1',\n",
       " 'pixel_0_2',\n",
       " 'pixel_0_3',\n",
       " 'pixel_0_4',\n",
       " 'pixel_0_5',\n",
       " 'pixel_0_6',\n",
       " 'pixel_0_7',\n",
       " 'pixel_1_0',\n",
       " 'pixel_1_1',\n",
       " 'pixel_1_2',\n",
       " 'pixel_1_3',\n",
       " 'pixel_1_4',\n",
       " 'pixel_1_5',\n",
       " 'pixel_1_6',\n",
       " 'pixel_1_7',\n",
       " 'pixel_2_0',\n",
       " 'pixel_2_1',\n",
       " 'pixel_2_2',\n",
       " 'pixel_2_3',\n",
       " 'pixel_2_4',\n",
       " 'pixel_2_5',\n",
       " 'pixel_2_6',\n",
       " 'pixel_2_7',\n",
       " 'pixel_3_0',\n",
       " 'pixel_3_1',\n",
       " 'pixel_3_2',\n",
       " 'pixel_3_3',\n",
       " 'pixel_3_4',\n",
       " 'pixel_3_5',\n",
       " 'pixel_3_6',\n",
       " 'pixel_3_7',\n",
       " 'pixel_4_0',\n",
       " 'pixel_4_1',\n",
       " 'pixel_4_2',\n",
       " 'pixel_4_3',\n",
       " 'pixel_4_4',\n",
       " 'pixel_4_5',\n",
       " 'pixel_4_6',\n",
       " 'pixel_4_7',\n",
       " 'pixel_5_0',\n",
       " 'pixel_5_1',\n",
       " 'pixel_5_2',\n",
       " 'pixel_5_3',\n",
       " 'pixel_5_4',\n",
       " 'pixel_5_5',\n",
       " 'pixel_5_6',\n",
       " 'pixel_5_7',\n",
       " 'pixel_6_0',\n",
       " 'pixel_6_1',\n",
       " 'pixel_6_2',\n",
       " 'pixel_6_3',\n",
       " 'pixel_6_4',\n",
       " 'pixel_6_5',\n",
       " 'pixel_6_6',\n",
       " 'pixel_6_7',\n",
       " 'pixel_7_0',\n",
       " 'pixel_7_1',\n",
       " 'pixel_7_2',\n",
       " 'pixel_7_3',\n",
       " 'pixel_7_4',\n",
       " 'pixel_7_5',\n",
       " 'pixel_7_6',\n",
       " 'pixel_7_7']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with `load_wine`, `load_breast_cancer`, `load_linerud` later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Breast Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_breast_cancer in module sklearn.datasets._base:\n",
      "\n",
      "load_breast_cancer(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the breast cancer wisconsin dataset (classification).\n",
      "    \n",
      "    The breast cancer dataset is a classic and very easy binary classification\n",
      "    dataset.\n",
      "    \n",
      "    =================   ==============\n",
      "    Classes                          2\n",
      "    Samples per class    212(M),357(B)\n",
      "    Samples total                  569\n",
      "    Dimensionality                  30\n",
      "    Features            real, positive\n",
      "    =================   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <breast_cancer_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (569, 30)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (569,)\n",
      "            The classification target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        target_names: list\n",
      "            The names of target classes.\n",
      "        frame: DataFrame of shape (569, 31)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        filename: str\n",
      "            The path to the location of the data.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
      "    downloaded from:\n",
      "    https://goo.gl/U2Uwz2\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let's say you are interested in the samples 10, 50, and 85, and want to\n",
      "    know their class name.\n",
      "    \n",
      "    >>> from sklearn.datasets import load_breast_cancer\n",
      "    >>> data = load_breast_cancer()\n",
      "    >>> data.target[[10, 50, 85]]\n",
      "    array([0, 1, 0])\n",
      "    >>> list(data.target_names)\n",
      "    ['malignant', 'benign']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "help(load_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_breast_cancer(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 212), (1, 357)]\n"
     ]
    }
   ],
   "source": [
    "uniques,counts = np.unique(y, return_counts=True)\n",
    "print(list(zip(uniques,counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcancer_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `'malignant'` corresponds to `0` and `'benign'` corresponds to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(bcancer_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching California Housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import the library and access the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_california_housing in module sklearn.datasets._california_housing:\n",
      "\n",
      "fetch_california_housing(*, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False)\n",
      "    Load the California housing dataset (regression).\n",
      "    \n",
      "    ==============   ==============\n",
      "    Samples total             20640\n",
      "    Dimensionality                8\n",
      "    Features                   real\n",
      "    Target           real 0.15 - 5.\n",
      "    ==============   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <california_housing_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : str, default=None\n",
      "        Specify another download and cache folder for the datasets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    download_if_missing : bool, default=True\n",
      "        If False, raise a IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    \n",
      "    return_X_y : bool, default=False.\n",
      "        If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric, string or categorical). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target_columns.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dataset : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : ndarray, shape (20640, 8)\n",
      "            Each row corresponding to the 8 feature values in order.\n",
      "            If ``as_frame`` is True, ``data`` is a pandas object.\n",
      "        target : numpy array of shape (20640,)\n",
      "            Each value corresponds to the average\n",
      "            house value in units of 100,000.\n",
      "            If ``as_frame`` is True, ``target`` is a pandas object.\n",
      "        feature_names : list of length 8\n",
      "            Array of ordered feature names used in the dataset.\n",
      "        DESCR : string\n",
      "            Description of the California housing dataset.\n",
      "        frame : pandas DataFrame\n",
      "            Only present when `as_frame=True`. DataFrame with ``data`` and\n",
      "            ``target``.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    \n",
      "    This dataset consists of 20,640 samples and 9 features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "help(fetch_california_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `fetch_*` also returns a `Bunch` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Load the dataset and obtain a `Bunch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the loader and obtain the `Bunch` object.\n",
    "housing_data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Examine the bunch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Description of the dataset.\n",
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the feature matrix\n",
    "housing_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
       "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
       "         3.78800000e+01, -1.22230000e+02],\n",
       "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
       "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
       "         3.78600000e+01, -1.22220000e+02],\n",
       "       [ 7.25740000e+00,  5.20000000e+01,  8.28813559e+00,\n",
       "         1.07344633e+00,  4.96000000e+02,  2.80225989e+00,\n",
       "         3.78500000e+01, -1.22240000e+02],\n",
       "       [ 5.64310000e+00,  5.20000000e+01,  5.81735160e+00,\n",
       "         1.07305936e+00,  5.58000000e+02,  2.54794521e+00,\n",
       "         3.78500000e+01, -1.22250000e+02],\n",
       "       [ 3.84620000e+00,  5.20000000e+01,  6.28185328e+00,\n",
       "         1.08108108e+00,  5.65000000e+02,  2.18146718e+00,\n",
       "         3.78500000e+01, -1.22250000e+02]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first 5 examples from the featrue matrix\n",
    "housing_data.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the features\n",
    "housing_data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the label matrix\n",
    "housing_data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, 3.413, 3.422])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedHouseVal']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the label\n",
    "housing_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching from `openml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[openml.org](openml.org) is a public repository for machine learning data and experiments, that allows everybody to upload open datasets.\n",
    "\n",
    "\n",
    "(Many popular datasets like the MNIST Digit dataset can be fetched from here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library and access the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_openml in module sklearn.datasets._openml:\n",
      "\n",
      "fetch_openml(name: Union[str, NoneType] = None, *, version: Union[str, int] = 'active', data_id: Union[int, NoneType] = None, data_home: Union[str, NoneType] = None, target_column: Union[str, List, NoneType] = 'default-target', cache: bool = True, return_X_y: bool = False, as_frame: Union[str, bool] = 'auto')\n",
      "    Fetch dataset from openml by name or dataset id.\n",
      "    \n",
      "    Datasets are uniquely identified by either an integer ID or by a\n",
      "    combination of name and version (i.e. there might be multiple\n",
      "    versions of the 'iris' dataset). Please give either name or data_id\n",
      "    (not both). In case a name is given, a version can also be\n",
      "    provided.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <openml>`.\n",
      "    \n",
      "    .. versionadded:: 0.20\n",
      "    \n",
      "    .. note:: EXPERIMENTAL\n",
      "    \n",
      "        The API is experimental (particularly the return value structure),\n",
      "        and might have small backward-incompatible changes without notice\n",
      "        or warning in future releases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str, default=None\n",
      "        String identifier of the dataset. Note that OpenML can have multiple\n",
      "        datasets with the same name.\n",
      "    \n",
      "    version : int or 'active', default='active'\n",
      "        Version of the dataset. Can only be provided if also ``name`` is given.\n",
      "        If 'active' the oldest version that's still active is used. Since\n",
      "        there may be more than one active version of a dataset, and those\n",
      "        versions may fundamentally be different from one another, setting an\n",
      "        exact version is highly recommended.\n",
      "    \n",
      "    data_id : int, default=None\n",
      "        OpenML ID of the dataset. The most specific way of retrieving a\n",
      "        dataset. If data_id is not given, name (and potential version) are\n",
      "        used to obtain a dataset.\n",
      "    \n",
      "    data_home : str, default=None\n",
      "        Specify another download and cache folder for the data sets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    target_column : str, list or None, default='default-target'\n",
      "        Specify the column name in the data to use as target. If\n",
      "        'default-target', the standard target column a stored on the server\n",
      "        is used. If ``None``, all columns are returned as data and the\n",
      "        target is ``None``. If list (of strings), all columns with these names\n",
      "        are returned as multi-target (Note: not all scikit-learn classifiers\n",
      "        can handle all types of multi-output combinations)\n",
      "    \n",
      "    cache : bool, default=True\n",
      "        Whether to cache downloaded datasets using joblib.\n",
      "    \n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "        below for more information about the `data` and `target` objects.\n",
      "    \n",
      "    as_frame : bool or 'auto', default='auto'\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric, string or categorical). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target_columns.\n",
      "        The Bunch will contain a ``frame`` attribute with the target and the\n",
      "        data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n",
      "        DataFrames or Series as describe above.\n",
      "    \n",
      "        If as_frame is 'auto', the data and target will be converted to\n",
      "        DataFrame or Series as if as_frame is set to True, unless the dataset\n",
      "        is stored in sparse format.\n",
      "    \n",
      "        .. versionchanged:: 0.24\n",
      "           The default value of `as_frame` changed from `False` to `'auto'`\n",
      "           in 0.24.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    \n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n",
      "            The feature matrix. Categorical features are encoded as ordinals.\n",
      "        target : np.array, pandas Series or DataFrame\n",
      "            The regression target or classification labels, if applicable.\n",
      "            Dtype is float if numeric, and object if categorical. If\n",
      "            ``as_frame`` is True, ``target`` is a pandas object.\n",
      "        DESCR : str\n",
      "            The full description of the dataset\n",
      "        feature_names : list\n",
      "            The names of the dataset columns\n",
      "        target_names: list\n",
      "            The names of the target columns\n",
      "    \n",
      "        .. versionadded:: 0.22\n",
      "    \n",
      "        categories : dict or None\n",
      "            Maps each categorical feature name to a list of values, such\n",
      "            that the value encoded as i is ith in the list. If ``as_frame``\n",
      "            is True, this is None.\n",
      "        details : dict\n",
      "            More metadata from OpenML\n",
      "        frame : pandas DataFrame\n",
      "            Only present when `as_frame=True`. DataFrame with ``data`` and\n",
      "            ``target``.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. note:: EXPERIMENTAL\n",
      "    \n",
      "            This interface is **experimental** and subsequent releases may\n",
      "            change attributes without notice (although there should only be\n",
      "            minor changes to ``data`` and ``target``).\n",
      "    \n",
      "        Missing values in the 'data' are represented as NaN's. Missing values\n",
      "        in 'target' are represented as NaN's (numerical target) or None\n",
      "        (categorical target)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "help(fetch_openml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is an experimental API and is likely to change in the future releases.\n",
    "\n",
    "> We use this API for loading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (70000, 784)\n",
      "Label vector shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "X,y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Label vector shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 70000 entries, 0 to 69999\n",
      "Columns: 784 entries, pixel1 to pixel784\n",
      "dtypes: float64(784)\n",
      "memory usage: 418.7 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6', 'pixel7',\n",
       "       'pixel8', 'pixel9', 'pixel10',\n",
       "       ...\n",
       "       'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779', 'pixel780',\n",
       "       'pixel781', 'pixel782', 'pixel783', 'pixel784'],\n",
       "      dtype='object', length=784)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, each sample has 784 features, each of which is a pixel\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exercise: `fetch_20newsgroup` and `fetch_kddcup99`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `make_regression`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us generate datasets for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_regression in module sklearn.datasets._samples_generator:\n",
      "\n",
      "make_regression(n_samples=100, n_features=100, *, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
      "    Generate a random regression problem.\n",
      "    \n",
      "    The input set can either be well conditioned (by default) or have a low\n",
      "    rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n",
      "    more details.\n",
      "    \n",
      "    The output is generated by applying a (potentially biased) random linear\n",
      "    regression model with `n_informative` nonzero regressors to the previously\n",
      "    generated input and some gaussian centered noise with some adjustable\n",
      "    scale.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <sample_generators>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int, default=100\n",
      "        The number of samples.\n",
      "    \n",
      "    n_features : int, default=100\n",
      "        The number of features.\n",
      "    \n",
      "    n_informative : int, default=10\n",
      "        The number of informative features, i.e., the number of features used\n",
      "        to build the linear model used to generate the output.\n",
      "    \n",
      "    n_targets : int, default=1\n",
      "        The number of regression targets, i.e., the dimension of the y output\n",
      "        vector associated with a sample. By default, the output is a scalar.\n",
      "    \n",
      "    bias : float, default=0.0\n",
      "        The bias term in the underlying linear model.\n",
      "    \n",
      "    effective_rank : int, default=None\n",
      "        if not None:\n",
      "            The approximate number of singular vectors required to explain most\n",
      "            of the input data by linear combinations. Using this kind of\n",
      "            singular spectrum in the input allows the generator to reproduce\n",
      "            the correlations often observed in practice.\n",
      "        if None:\n",
      "            The input set is well conditioned, centered and gaussian with\n",
      "            unit variance.\n",
      "    \n",
      "    tail_strength : float, default=0.5\n",
      "        The relative importance of the fat noisy tail of the singular values\n",
      "        profile if `effective_rank` is not None. When a float, it should be\n",
      "        between 0 and 1.\n",
      "    \n",
      "    noise : float, default=0.0\n",
      "        The standard deviation of the gaussian noise applied to the output.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Shuffle the samples and the features.\n",
      "    \n",
      "    coef : bool, default=False\n",
      "        If True, the coefficients of the underlying linear model are returned.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Determines random number generation for dataset creation. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : ndarray of shape (n_samples, n_features)\n",
      "        The input samples.\n",
      "    \n",
      "    y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "        The output values.\n",
      "    \n",
      "    coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "        The coefficient of the underlying linear model. It is returned only if\n",
      "        coef is True.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "help(make_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Generating 100 samples with 5 features for a single label regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_regression(n_samples= 100, n_features= 5, n_targets=1, shuffle= True, random_state=42)\n",
    "\n",
    "# It is a good practice to set seed so that we get to see repeatability in the experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of feature matrix and label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93782504,  0.51504769,  0.51503527,  3.85273149,  0.51378595],\n",
       "       [ 1.0889506 , -0.71530371,  0.06428002,  0.67959775, -1.07774478],\n",
       "       [-0.60170661, -1.05771093,  1.85227818,  0.82254491, -0.01349722],\n",
       "       [ 0.8219025 ,  0.09176078,  0.08704707, -1.98756891, -0.29900735],\n",
       "       [ 1.54993441,  0.81351722, -0.78325329, -1.23086432, -0.32206152]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([271.31612081,   6.2305406 ,  11.86102446, -63.94057571,\n",
       "        49.63008485])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Generating 100 samples with 5 features for multiple regression problem with 5 outputs (each label 5-dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_regression(n_samples=100, n_features=5, n_targets=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Since, we generated multi-output target with 5 outputs, the output has shape `(100, 5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88163976, -1.42225371,  1.68714164, -0.64657288, -1.081548  ],\n",
       "       [ 1.30547881, -0.2176812 ,  0.81350964,  1.09877685,  0.82541635],\n",
       "       [ 0.13074058, -1.24778318, -0.44004449,  1.6324113 , -1.43014138],\n",
       "       [ 0.58685709,  0.79103195, -1.40185106, -0.90938745,  1.40279431],\n",
       "       [ 0.35701549, -0.20812225,  0.8496021 , -0.49300093, -0.58936476]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  46.58248846,   26.95176072,  158.49687583, -211.55965424,\n",
       "         -23.03343135],\n",
       "       [ 196.63689114,  168.66737136,  209.93926636,  122.35430764,\n",
       "          65.76629146],\n",
       "       [ -78.95894162, -102.12190675,  -76.82170915, -212.75380369,\n",
       "         -76.10553622],\n",
       "       [  39.05790167,   69.14878192,  -51.17607712,  180.89618908,\n",
       "          -1.18306497],\n",
       "       [  43.04031257,   32.06778878,   87.46866782,  -66.78313107,\n",
       "          14.64502385]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `make_classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random *n-class* classification problem, with a single label output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_classification in module sklearn.datasets._samples_generator:\n",
      "\n",
      "make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
      "    Generate a random n-class classification problem.\n",
      "    \n",
      "    This initially creates clusters of points normally distributed (std=1)\n",
      "    about vertices of an ``n_informative``-dimensional hypercube with sides of\n",
      "    length ``2*class_sep`` and assigns an equal number of clusters to each\n",
      "    class. It introduces interdependence between these features and adds\n",
      "    various types of further noise to the data.\n",
      "    \n",
      "    Without shuffling, ``X`` horizontally stacks features in the following\n",
      "    order: the primary ``n_informative`` features, followed by ``n_redundant``\n",
      "    linear combinations of the informative features, followed by ``n_repeated``\n",
      "    duplicates, drawn randomly with replacement from the informative and\n",
      "    redundant features. The remaining features are filled with random noise.\n",
      "    Thus, without shuffling, all useful features are contained in the columns\n",
      "    ``X[:, :n_informative + n_redundant + n_repeated]``.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <sample_generators>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int, default=100\n",
      "        The number of samples.\n",
      "    \n",
      "    n_features : int, default=20\n",
      "        The total number of features. These comprise ``n_informative``\n",
      "        informative features, ``n_redundant`` redundant features,\n",
      "        ``n_repeated`` duplicated features and\n",
      "        ``n_features-n_informative-n_redundant-n_repeated`` useless features\n",
      "        drawn at random.\n",
      "    \n",
      "    n_informative : int, default=2\n",
      "        The number of informative features. Each class is composed of a number\n",
      "        of gaussian clusters each located around the vertices of a hypercube\n",
      "        in a subspace of dimension ``n_informative``. For each cluster,\n",
      "        informative features are drawn independently from  N(0, 1) and then\n",
      "        randomly linearly combined within each cluster in order to add\n",
      "        covariance. The clusters are then placed on the vertices of the\n",
      "        hypercube.\n",
      "    \n",
      "    n_redundant : int, default=2\n",
      "        The number of redundant features. These features are generated as\n",
      "        random linear combinations of the informative features.\n",
      "    \n",
      "    n_repeated : int, default=0\n",
      "        The number of duplicated features, drawn randomly from the informative\n",
      "        and the redundant features.\n",
      "    \n",
      "    n_classes : int, default=2\n",
      "        The number of classes (or labels) of the classification problem.\n",
      "    \n",
      "    n_clusters_per_class : int, default=2\n",
      "        The number of clusters per class.\n",
      "    \n",
      "    weights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None\n",
      "        The proportions of samples assigned to each class. If None, then\n",
      "        classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n",
      "        then the last class weight is automatically inferred.\n",
      "        More than ``n_samples`` samples may be returned if the sum of\n",
      "        ``weights`` exceeds 1. Note that the actual class proportions will\n",
      "        not exactly match ``weights`` when ``flip_y`` isn't 0.\n",
      "    \n",
      "    flip_y : float, default=0.01\n",
      "        The fraction of samples whose class is assigned randomly. Larger\n",
      "        values introduce noise in the labels and make the classification\n",
      "        task harder. Note that the default setting flip_y > 0 might lead\n",
      "        to less than ``n_classes`` in y in some cases.\n",
      "    \n",
      "    class_sep : float, default=1.0\n",
      "        The factor multiplying the hypercube size.  Larger values spread\n",
      "        out the clusters/classes and make the classification task easier.\n",
      "    \n",
      "    hypercube : bool, default=True\n",
      "        If True, the clusters are put on the vertices of a hypercube. If\n",
      "        False, the clusters are put on the vertices of a random polytope.\n",
      "    \n",
      "    shift : float, ndarray of shape (n_features,) or None, default=0.0\n",
      "        Shift features by the specified value. If None, then features\n",
      "        are shifted by a random value drawn in [-class_sep, class_sep].\n",
      "    \n",
      "    scale : float, ndarray of shape (n_features,) or None, default=1.0\n",
      "        Multiply features by the specified value. If None, then features\n",
      "        are scaled by a random value drawn in [1, 100]. Note that scaling\n",
      "        happens after shifting.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Shuffle the samples and the features.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Determines random number generation for dataset creation. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : ndarray of shape (n_samples, n_features)\n",
      "        The generated samples.\n",
      "    \n",
      "    y : ndarray of shape (n_samples,)\n",
      "        The integer labels for class membership of each sample.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The algorithm is adapted from Guyon [1] and was designed to generate\n",
      "    the \"Madelon\" dataset.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
      "           selection benchmark\", 2003.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    make_blobs : Simplified variant.\n",
      "    make_multilabel_classification : Unrelated generator for multilabel tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "help(make_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Generate 100 samples for a binary classification problem with 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=100, n_features=10, n_classes=2, n_clusters_per_class=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the feature matrix and the label vector.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11422765, -1.71016839, -0.06822216, -0.14928517,  0.30780177,\n",
       "         0.15030176, -0.05694562, -0.22595246, -0.36361221, -0.13818757],\n",
       "       [ 0.70775194, -1.57022472, -0.23503183, -0.63604713,  0.62180996,\n",
       "        -0.56246678,  0.97255445, -0.77719676,  0.63240774, -0.47809669],\n",
       "       [ 0.63859246,  0.04739867,  0.33273433,  1.1046981 , -0.65183611,\n",
       "        -1.66152006, -1.2110162 ,  1.09821151, -0.0660798 ,  0.68024225],\n",
       "       [-0.23894805, -0.97755524,  0.0379061 ,  0.19896733,  0.50091719,\n",
       "        -0.90756366,  0.75539123,  0.12437227, -0.57677133,  0.07871283],\n",
       "       [-0.59239392, -0.05023811,  0.17573204, -1.43949185,  0.27045683,\n",
       "        -0.86399077, -0.83095012,  0.60046915,  0.04852163,  0.32557953]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, this is a binary classification problem, we only have 2 outputs 0 and 1.\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "Generate a 3 classification problem with 100 samples and 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=100, n_features=10, n_classes=3, n_clusters_per_class=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the feature matrix and the label vector.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58351628, -1.73833907, -1.37298251, -1.77311485,  0.45918008,\n",
       "         0.83392215, -1.66096093,  0.20768769, -0.07016571,  0.42961822],\n",
       "       [-1.0044394 , -1.43862044,  0.47335819, -0.21188291,  0.0125924 ,\n",
       "         0.22409248, -0.77300978,  0.49799829,  0.0976761 ,  0.02451017],\n",
       "       [ 0.07740833,  0.19896733,  0.12437227,  0.17738132, -0.97755524,\n",
       "         0.50091719,  0.75138712,  0.54336019,  0.09933231, -1.66940528],\n",
       "       [-0.91759569, -0.9609536 ,  1.07746664,  0.4522739 , -0.32138584,\n",
       "        -0.8254972 , -0.56372455,  0.24368721,  0.41293145, -0.8222204 ],\n",
       "       [-0.96222828, -0.96090774,  1.21530116,  0.55980482, -1.24778318,\n",
       "        -0.25256815, -1.43014138,  0.13074058,  1.6324113 , -0.44004449]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, this is a 3 classification problem, we only have 3 outputs 0, 1 and 2.\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `make_multilabel_classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps us in generating a random multi-label classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_multilabel_classification in module sklearn.datasets._samples_generator:\n",
      "\n",
      "make_multilabel_classification(n_samples=100, n_features=20, *, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\n",
      "    Generate a random multilabel classification problem.\n",
      "    \n",
      "    For each sample, the generative process is:\n",
      "        - pick the number of labels: n ~ Poisson(n_labels)\n",
      "        - n times, choose a class c: c ~ Multinomial(theta)\n",
      "        - pick the document length: k ~ Poisson(length)\n",
      "        - k times, choose a word: w ~ Multinomial(theta_c)\n",
      "    \n",
      "    In the above process, rejection sampling is used to make sure that\n",
      "    n is never zero or more than `n_classes`, and that the document length\n",
      "    is never zero. Likewise, we reject classes which have already been chosen.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <sample_generators>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int, default=100\n",
      "        The number of samples.\n",
      "    \n",
      "    n_features : int, default=20\n",
      "        The total number of features.\n",
      "    \n",
      "    n_classes : int, default=5\n",
      "        The number of classes of the classification problem.\n",
      "    \n",
      "    n_labels : int, default=2\n",
      "        The average number of labels per instance. More precisely, the number\n",
      "        of labels per sample is drawn from a Poisson distribution with\n",
      "        ``n_labels`` as its expected value, but samples are bounded (using\n",
      "        rejection sampling) by ``n_classes``, and must be nonzero if\n",
      "        ``allow_unlabeled`` is False.\n",
      "    \n",
      "    length : int, default=50\n",
      "        The sum of the features (number of words if documents) is drawn from\n",
      "        a Poisson distribution with this expected value.\n",
      "    \n",
      "    allow_unlabeled : bool, default=True\n",
      "        If ``True``, some instances might not belong to any class.\n",
      "    \n",
      "    sparse : bool, default=False\n",
      "        If ``True``, return a sparse feature matrix\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           parameter to allow *sparse* output.\n",
      "    \n",
      "    return_indicator : {'dense', 'sparse'} or False, default='dense'\n",
      "        If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n",
      "        ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n",
      "        ``False`` returns a list of lists of labels.\n",
      "    \n",
      "    return_distributions : bool, default=False\n",
      "        If ``True``, return the prior class probability and conditional\n",
      "        probabilities of features given classes, from which the data was\n",
      "        drawn.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Determines random number generation for dataset creation. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : ndarray of shape (n_samples, n_features)\n",
      "        The generated samples.\n",
      "    \n",
      "    Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "        The label sets. Sparse matrix should be of CSR format.\n",
      "    \n",
      "    p_c : ndarray of shape (n_classes,)\n",
      "        The probability of each class being drawn. Only returned if\n",
      "        ``return_distributions=True``.\n",
      "    \n",
      "    p_w_c : ndarray of shape (n_features, n_classes)\n",
      "        The probability of each feature being drawn given each class.\n",
      "        Only returned if ``return_distributions=True``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "help(make_multilabel_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Generate 100 multilabel classification samples with 10 features, 5 labels and on average 2 labels per examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (what does average of 2 labels per example mean? \n",
    "# this is set by n_labels=2 argument ---)\n",
    "\n",
    "X,y = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5, n_labels=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(100, 5)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of the feature matrix and the label vector.\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  8.,  5.,  2.,  0.,  1.,  1.,  3.,  6.,  6.],\n",
       "       [ 4.,  5.,  8.,  4.,  3.,  5.,  2.,  0.,  2., 13.],\n",
       "       [ 7.,  5.,  5.,  1.,  1.,  9.,  3.,  2.,  3.,  8.],\n",
       "       [10.,  4.,  9.,  5.,  7.,  7.,  2.,  4.,  4.,  0.],\n",
       "       [ 9., 14.,  5.,  0.,  5.,  0.,  2.,  0.,  2., 10.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these label mean?\n",
    "In output 0 (aka `y[0] = array([0,1,0,0,0])`) this means (class-0 is absent, class-1 is present, class-2 is absent, class-3 is absent, class-4 is absent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `make_blobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables us to generate random data for clustering (for unsupervised learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_blobs in module sklearn.datasets._samples_generator:\n",
      "\n",
      "make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None, return_centers=False)\n",
      "    Generate isotropic Gaussian blobs for clustering.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <sample_generators>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int or array-like, default=100\n",
      "        If int, it is the total number of points equally divided among\n",
      "        clusters.\n",
      "        If array-like, each element of the sequence indicates\n",
      "        the number of samples per cluster.\n",
      "    \n",
      "        .. versionchanged:: v0.20\n",
      "            one can now pass an array-like to the ``n_samples`` parameter\n",
      "    \n",
      "    n_features : int, default=2\n",
      "        The number of features for each sample.\n",
      "    \n",
      "    centers : int or ndarray of shape (n_centers, n_features), default=None\n",
      "        The number of centers to generate, or the fixed center locations.\n",
      "        If n_samples is an int and centers is None, 3 centers are generated.\n",
      "        If n_samples is array-like, centers must be\n",
      "        either None or an array of length equal to the length of n_samples.\n",
      "    \n",
      "    cluster_std : float or array-like of float, default=1.0\n",
      "        The standard deviation of the clusters.\n",
      "    \n",
      "    center_box : tuple of float (min, max), default=(-10.0, 10.0)\n",
      "        The bounding box for each cluster center when centers are\n",
      "        generated at random.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Shuffle the samples.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Determines random number generation for dataset creation. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    return_centers : bool, default=False\n",
      "        If True, then return the centers of each cluster\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : ndarray of shape (n_samples, n_features)\n",
      "        The generated samples.\n",
      "    \n",
      "    y : ndarray of shape (n_samples,)\n",
      "        The integer labels for cluster membership of each sample.\n",
      "    \n",
      "    centers : ndarray of shape (n_centers, n_features)\n",
      "        The centers of each cluster. Only returned if\n",
      "        ``return_centers=True``.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import make_blobs\n",
      "    >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n",
      "    ...                   random_state=0)\n",
      "    >>> print(X.shape)\n",
      "    (10, 2)\n",
      "    >>> y\n",
      "    array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n",
      "    >>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,\n",
      "    ...                   random_state=0)\n",
      "    >>> print(X.shape)\n",
      "    (10, 2)\n",
      "    >>> y\n",
      "    array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    make_classification : A more intricate variant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "help(make_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Generate a random dataset of 10 samples with 2 features each for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=10, centers=3, n_features=2, random_state=42)\n",
    "# Here, the number of centres help us generate datasets from those manyy clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 2, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can find the cluster membership of each point in y. Since, we have centres = 3, we have 3 clusters namely 0,1 and 2.\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further examples refer this notebook ---\n",
    "\n",
    "[MLP_SWI_Week_1.ipynb](file:///Users/sampadk04/Desktop/Programming/VSCode-Projects/Python/IITM/IITM-MLP/SWI/MLP_SWI_Week_1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0b4aa3dbf26008a2f5bdb6f96959d9b12aab8bbe92ce96b5c8a3811effcf69d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('skk-mlp-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
