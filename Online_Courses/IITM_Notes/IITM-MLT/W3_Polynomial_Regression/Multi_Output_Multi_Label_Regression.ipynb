{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "# This is imported for proper rendering of Latex in Notebook.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import for generating plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Output/Multi-Label Regression\n",
    "\n",
    "In case of multi-output regression, there are more than one output labels, all of which are real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "$D = (X,Y) = \\{(x^{(i)},y^{(i)})\\}^{n}_{i=0}$\n",
    "\n",
    "Here $y^{(i)} \\in \\mathbb{R}^k$ where $k$ is the no. of output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data\n",
    "\n",
    "Let's generate synthetic data for demonstrating the training set in multi-output regression using `sklearn.datasets.make_regression` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X,Y,coef = make_regression(n_samples=100, n_features=10, n_informative=10, bias=1, n_targets=5, shuffle=True, coef=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10) (100, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the first $5$ examples in terms of their features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training examples:\n",
      " [[-2.07339023 -0.37144087  1.27155509  1.75227044  0.93567839 -1.40751169\n",
      "  -0.77781669 -0.34268759 -1.11057585  1.24608519]\n",
      " [-0.90938745 -1.40185106 -0.50347565 -0.56629773  0.09965137  0.58685709\n",
      "   2.19045563  1.40279431 -0.99053633  0.79103195]\n",
      " [-0.18565898 -1.19620662 -0.64511975  1.0035329   0.36163603  0.81252582\n",
      "   1.35624003 -1.10633497 -0.07201012 -0.47917424]\n",
      " [ 0.03526355  0.21397991 -0.57581824  0.75750771 -0.53050115 -0.11232805\n",
      "  -0.2209696  -0.69972551  0.6141667   1.96472513]\n",
      " [-0.51604473 -0.46227529 -0.8946073  -0.47874862  1.25575613 -0.43449623\n",
      "  -0.30917212  0.09612078  0.22213377  0.93828381]]\n",
      "Corresponding labels:\n",
      " [[-133.15919852  -88.95797818   98.19127175   25.68295511 -132.79294654]\n",
      " [-110.38909784  146.04459736 -169.58916067  118.96066861 -177.08414159]\n",
      " [ -97.80350267    4.32654061  -87.56082281   -5.58466452    6.36897388]\n",
      " [  25.39024616  -70.41180117  186.15213706  132.77153362   53.42301307]\n",
      " [-140.61925153  -53.87007831 -101.11514549 -113.36926374 -115.61959345]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample training examples:\\n\", X[:5])\n",
    "print(\"Corresponding labels:\\n\", Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the coefficients or weight vectors used for generating this dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(coef)\n",
    "print(coef.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we have $5$ labels per output and each each input has $10$ features, we get a `(10,5)` weight vector signifying that we have 10 weight coefficients per output label.\n",
    "\n",
    "This in some sense tells us that this is going to be equivalent to running linear regression $5$ times, once per output label and combinig the weight vector in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Dummy feature and train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_feature(X:np.ndarray):\n",
    "    ''' Adds a dummy feature to the dataset.\n",
    "    \n",
    "    Args:\n",
    "        X: Training dataset\n",
    "    \n",
    "    Returns:\n",
    "        Training dataset with the addition of dummy feature.\n",
    "    '''\n",
    "    return np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "def preprocess(X:np.ndarray,y:np.ndarray):\n",
    "    ''' Adds a dummy feature to the dataset. Then splits the dataset into 2 parts --- 80% for training and 20% for testing.\n",
    "    \n",
    "    Args:\n",
    "        X: Training dataset\n",
    "        y: Label vector\n",
    "    \n",
    "    Returns:\n",
    "        Training dataset with the addition of dummy feature.\n",
    "        Testing dataset with the addition of dummy feature.\n",
    "        Training label vector.\n",
    "        Testing label vector.\n",
    "    '''\n",
    "    X_with_dummy_features = add_dummy_feature(X)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    return train_test_split(X_with_dummy_features, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = preprocess(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 11) (20, 11)\n",
      "(80, 5) (20, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model is exactly the same as a single label output except that the output now is a vector:\n",
    "$$Y_{n \\times k} = X_{n \\times (m+1)} W_{(m+1) \\times k}$$\n",
    "\n",
    "In this equation, the output label is a matrix $Y$. In order to generate multiple outputs, we nee one weight vector per output. Hence, total of $k$ weight vectors corresponding to the $k$ outputs.\n",
    "\n",
    "There are $2$ options for modelling this problem:\n",
    "1. Solve $k$ independent linear regression problems. Gives some flexibility in using different representation for each problem.\n",
    "2. Solve a joint learning problem as outined in the equation above. We will pursue this approach in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "We use the same loss function as the linear regression i.e. sum of squared error.\n",
    "$$J(W) = \\frac{1}{2} (XW - Y)^T (XW - Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "1. Normal Equation\n",
    "2. Gradient descent and its variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "RMSE or Squared Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Modified Linear Regression model\n",
    "    -----------------------\n",
    "    y = X@w\n",
    "    X: Feature matrix\n",
    "    w: Weight vector\n",
    "    y: Label vector\n",
    "    Î»: Regularization Rate\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "    \n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Prediction of output label for a given input.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "        \n",
    "        Returns:\n",
    "            y: Output label vector as predicted by the given model\n",
    "        '''\n",
    "\n",
    "        y = X @ self.w\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, X:np.ndarray, y:np.ndarray, reg_rate:float=0) -> float:\n",
    "        '''Calculate loss for a model based on actual labels.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as known from the dataset (actual labels)\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        e = y - self.predict(X)\n",
    "        # return (1/2) * (np.transpose(e) @ e)\n",
    "        return (1/2) * (np.transpose(e) @ e) + (reg_rate/2) * (np.transpose(self.w) @ self.w)\n",
    "    \n",
    "    def rmse(self, X:np.ndarray, y:np.ndarray, reg_rate:float=0) -> float:\n",
    "        '''Calculate root mean squared error of prediction based on actual labels.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as known from the dataset (actual label)\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            RMSE\n",
    "        '''\n",
    "        return np.sqrt((2/X.shape[0]) * self.loss(X,y,reg_rate))\n",
    "    \n",
    "    # Normal Equation based estimation\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, reg_rate:float=0) -> np.ndarray:\n",
    "        '''Estimate parameters of linear regression model with normal equation.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as known from the dataset (actual label)\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            Weight vector via normal equation\n",
    "        '''\n",
    "        # self.w = np.linalg.pinv(X) @ y\n",
    "\n",
    "        self.w = np.zeros((X.shape[1],y.shape[1]))\n",
    "        eye = np.eye(np.size(X,1))\n",
    "        self.w = np.linalg.solve(\n",
    "            reg_rate * eye + X.T @ X,\n",
    "            X.T @ y,\n",
    "        )\n",
    "\n",
    "        return self.w\n",
    "    \n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray, reg_rate:float=0) -> np.ndarray:\n",
    "        '''Calculate gradients of loss function w.r.t. weight vector on training set.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as known from the dataset (actual label)\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            A vector of gradients\n",
    "        '''\n",
    "        return np.transpose(X) @ (self.predict(X) - y) + reg_rate * self.w\n",
    "    \n",
    "    def update_weights(self, grad:np.ndarray, lr:float) -> np.ndarray:\n",
    "        '''Update the weights based on the gradient of the loss function.\n",
    "\n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old - lr * grad\n",
    "        \n",
    "        Args:\n",
    "            grad: Gradient of loss w.r.t w\n",
    "            lr: Learning rate\n",
    "        \n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "        return self.w - lr * grad\n",
    "    \n",
    "    # Dynamic learning rate\n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0/(t + self.t1)\n",
    "    \n",
    "    # GD - Gradient Descent\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate:float=0) -> np.ndarray:\n",
    "        '''Estimates parameters of linar regression model through gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Output label for training data vector\n",
    "            num_epochs: Number of training steps\n",
    "            lr: Learning rate\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1],y.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        \n",
    "        for i in np.arange(0, num_epochs):\n",
    "            dJdW = self.calculate_gradient(X,y,reg_rate)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X,y,reg_rate))\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "        \n",
    "        return self.w\n",
    "    \n",
    "    # MBGD - Mini-Batch Gradient Descent\n",
    "    def mbgd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, batch_size:int, reg_rate:float=0) -> np.ndarray:\n",
    "        '''Estimates parameters of linar regression model through mini-batch gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Output label for training data vector\n",
    "            num_epochs: No. of epochs (no. of times MBGD is done over the whole training set)\n",
    "            batch_size: Size of each mini-batch, after which we update the weights\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1],y.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                \n",
    "                # ith mini-batch\n",
    "                Xi = X_shuffled[i:i + batch_size]\n",
    "                yi = y_shuffled[i:i + batch_size]\n",
    "\n",
    "                lr = self.learning_schedule(mini_batch_id)\n",
    "                gradients = (2/ batch_size) * self.calculate_gradient(Xi, yi, reg_rate)\n",
    "\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(Xi, yi, reg_rate))\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "        \n",
    "        return self.w\n",
    "    \n",
    "    # SGD - Stochastic Gradient Descent\n",
    "    def sgd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, reg_rate:float=0) -> np.ndarray:\n",
    "        '''Estimates parameters of linar regression model through stochastic gradient descent.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Output label for training data vector\n",
    "            num_epochs: No. of epochs (no. of times MBGD is done over the whole training set)\n",
    "            reg_rate: Rate of regularization\n",
    "        \n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1],y.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                Xi = X[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] + i)\n",
    "                gradients = 2 * self.calculate_gradient(Xi, yi, reg_rate)\n",
    "\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(Xi,yi,reg_rate))\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "        \n",
    "        return self.w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the estimated weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinReg()\n",
    "W = lin_reg.fit(X_train,Y_train)\n",
    "\n",
    "# Check if the weight vector is same as the coefficient vector used for making the data:\n",
    "np.testing.assert_almost_equal(W[1:,:], coef, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n",
      "Original Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Weights: ',W[1:,:]) # The first row only contains the bias term = 1. We are ignoring that.\n",
    "\n",
    "print('Original Weights: ',coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinReg()\n",
    "W = lin_reg.gd(X_train, Y_train, num_epochs=1000, lr=0.01)\n",
    "\n",
    "# Check if the weight vector is same as the coefficient vector used for making the data:\n",
    "np.testing.assert_almost_equal(W[1:,:], coef, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n",
      "Original Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Weights: ',W[1:,:]) # The first row only contains the bias term = 1. We are ignoring that.\n",
    "\n",
    "print('Original Weights: ',coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 2 decimals\n\nMismatched elements: 16 / 50 (32%)\nMax absolute difference: 0.0346066\nMax relative difference: 0.00752033\n x: array([[93.62,  5.16, 54.12, 70.88, 87.08],\n       [89.49, 54.79, 81.74, 45.25, 64.38],\n       [46.26, 86.8 , 72.71, 74.25, 42.53],...\n y: array([[93.62,  5.2 , 54.13, 70.91, 87.1 ],\n       [89.48, 54.76, 81.73, 45.23, 64.36],\n       [46.26, 86.83, 72.72, 74.27, 42.55],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-5aadc16adf88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Check if the weight vector is same as the coefficient vector used for making the data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;32m/Volumes/SKK-T7/Apps/anaconda3/envs/skk-mlp-env/lib/python3.8/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    840\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 2 decimals\n\nMismatched elements: 16 / 50 (32%)\nMax absolute difference: 0.0346066\nMax relative difference: 0.00752033\n x: array([[93.62,  5.16, 54.12, 70.88, 87.08],\n       [89.49, 54.79, 81.74, 45.25, 64.38],\n       [46.26, 86.8 , 72.71, 74.25, 42.53],...\n y: array([[93.62,  5.2 , 54.13, 70.91, 87.1 ],\n       [89.48, 54.76, 81.73, 45.23, 64.36],\n       [46.26, 86.83, 72.72, 74.27, 42.55],..."
     ]
    }
   ],
   "source": [
    "lin_reg = LinReg()\n",
    "W = lin_reg.mbgd(X_train, Y_train, num_epochs=1000, batch_size=20)\n",
    "\n",
    "# Check if the weight vector is same as the coefficient vector used for making the data:\n",
    "np.testing.assert_almost_equal(W[1:,:], coef, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They match till $1$ decimal place, but not after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weights:  [[93.61514028  5.16252176 54.12227532 70.88141712 87.07548692]\n",
      " [89.48653602 54.78810686 81.73565699 45.25259563 64.37552893]\n",
      " [46.25744465 86.79764942 72.71030751 74.24925631 42.53117509]\n",
      " [71.92281495 22.87213671 99.63808298 97.49644353 65.04727429]\n",
      " [19.94744496 67.9928184   7.21331831  3.04219854 25.74835101]\n",
      " [52.64218951 73.16373962  8.16440679  6.04014815 24.71431516]\n",
      " [15.95142867 87.15563324 21.91781306 97.57233239 33.67660043]\n",
      " [71.40912191 80.17792989 33.9466023  81.48618571  8.01437577]\n",
      " [18.20923554 78.94685452 65.86733639 49.80569078 55.52366644]\n",
      " [16.74906992 10.45737682 63.64129234 70.64605449  3.15901489]]\n",
      "Original Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Weights: ',W[1:,:]) # The first row only contains the bias term = 1. We are ignoring that.\n",
    "\n",
    "print('Original Weights: ',coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinReg()\n",
    "W = lin_reg.sgd(X_train, Y_train, num_epochs=1000)\n",
    "\n",
    "# Check if the weight vector is same as the coefficient vector used for making the data:\n",
    "np.testing.assert_almost_equal(W[1:,:], coef, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n",
      "Original Weights:  [[93.62122462  5.19712837 54.12963353 70.90605195 87.09691237]\n",
      " [89.48166561 54.75923762 81.729777   45.23182845 64.35776952]\n",
      " [46.26229567 86.82725054 72.71690698 74.27065212 42.54933344]\n",
      " [71.92017783 22.84547413 99.63339161 97.47931621 65.03256863]\n",
      " [19.95424509 68.02282424  7.2198409   3.06525022 25.76828885]\n",
      " [52.64026609 73.15895218  8.1629982   6.0352084  24.7103234 ]\n",
      " [15.95446801 87.17835666 21.92139874 97.58652558 33.68957918]\n",
      " [71.40869321 80.17280831 33.94501925 81.48251137  8.01148464]\n",
      " [18.21179157 78.96985071 65.87077755 49.81957165 55.53635509]\n",
      " [16.74825823 10.45678403 63.64302495 70.64757265  3.15861448]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted Weights: ',W[1:,:]) # The first row only contains the bias term = 1. We are ignoring that.\n",
    "\n",
    "print('Original Weights: ',coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do :**\n",
    "1. Plot graphs of Loss vs Iteration, using some other metric of loss for GD, MBGD, SGD.\n",
    "\n",
    "2. Solve multi-label regression problem as $k$-independent single label regression problems.\n",
    "\n",
    "3. Generate dataset with non-linear relationship between input and output. And fit a polynomial regression model with appropriate regularization."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0b4aa3dbf26008a2f5bdb6f96959d9b12aab8bbe92ce96b5c8a3811effcf69d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('skk-mlp-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
